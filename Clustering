#####################################################################
#                      import libraries                             #
#####################################################################
from pyspark.ml.feature import Tokenizer, RegexTokenizer
from pyspark.sql.functions import col, udf
from pyspark.sql.types import IntegerType
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer
from pyspark.sql.functions import monotonically_increasing_id
from pyspark.mllib.clustering import LDA, LDAModel
from pyspark.sql import Row
from pyspark.sql import SQLContext
from pyspark.ml.feature import Tokenizer, RegexTokenizer
from pyspark.ml.feature import StopWordsRemover
from nltk.corpus import stopwords
from shutil import copyfile
import os
import math
from pyspark.sql.functions import monotonically_increasing_id

import json


import re # needed to remove special character
from pyspark import Row
from pyspark.ml.feature import StopWordsRemover
from pyspark.ml.feature import Tokenizer, CountVectorizer
from pyspark.mllib.clustering import LDA
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, LongType

#####################################################################
#                        function definitions                       #
#####################################################################

# return max index of a list l
def maxList(l): return l.index(max(l))

# specify vector id of words to actual words
def topic_render(topic, words, woerter):  # specify vector id of words to actual words
    wordNumbers=woerter
    vocabArray=words
    terms = topic[0]    
    result = []
    if wordNumbers > len(terms): 
        maxIndex=len(terms)
    else: maxIndex=wordNumbers
    for t in range(maxIndex):
        term = vocabArray[terms[t]]        
        result.append(term)
        print result
    return result
#Saves the topTopics into Directory in a .txt file
def saveTopTopics(neu, newFile, topTops, name, number):
    a = neu + "/" + name + "/" + "topics" + number + newFile, "w"
    print a
    text_file = open(neu + "/" + name + "/" + "topics" + number + newFile, "w")
    myText = " ".join(topTops[0])
    text_file.write(myText)
    text_file.close()

#creates new directories for documents and topTopics
def createDirectories(dirName, numCluster, name, words):
    wordNumbers = words
    #dirName=clusterDestination
    #numCluster=numberClusters
    #name=index
    for b in range(numCluster):
                myTopicId="/" + str(b)
                NewDirectoryName=dirName + "/" + name + "/" + "documents" + myTopicId
                NewDirectoryTopics=dirName + "/" + name + "/" + "topics" + myTopicId
                if not os.path.exists(NewDirectoryName):
                    os.makedirs(NewDirectoryName)
                    print NewDirectoryName + " was created"
                if not os.path.exists(NewDirectoryTopics):
                    os.makedirs(NewDirectoryTopics)
                    print NewDirectoryTopics + " was created"
                    
                    
def assignment(assignList, numberClusters):
  
    assign = []
    docList = [] 
    documentList = []

    for j in range(0, numberClusters):
        docList = []
        docList.append(str(j))
        for i in range(len(assignList)):
            if (assignList[i][0] == str(j)):
                documentList.append(assignList[i][1])
        docList.append(documentList)
        documentList= []
        assign.append(docList)    
    print "assign"  
    print assign
    return assign
                 
                                              
                     
def jsonOutput(topics_final, scorelist, ldaModelPath, assignmentTopics, numberClusters, ldaIterations, wordNumbers, path):
    data = dict()
       
    ldaModel = dict()
    ldaModel["ldaPath"] = ldaModelPath
    ldaModel["documentsPath"] = path
    ldaModel["datetime"] = "zeit fehlt noch"
    ldaModel["numberCluster"] = numberClusters
    ldaModel["ldaIterations"] = ldaIterations
    ldaModel["wordNumbers"] = wordNumbers
    
    
    data["ldaModel"] = ldaModel
    
     
    
    
    topicsDic = dict()
    
    for i in range(0,len(topics_final)):
            topics = dict()
            topic = topics_final[i] 
            topics["Words" + str(i)] = topic    
            
            
            score = scorelist[i]
            topics["ScoresTopic" + str(i)] = score
            
            for j in range(0,len(assignmentTopics)):
                if (assignmentTopics[j][0] == str(i)):
                    topics["Documents" + str(i)] = assignmentTopics[j][1]
                    
            topicsDic["Topic" + str(i)] = topics       
            data["topics"] = topicsDic    
    
    zeichenkette = json.dumps(data , indent = 4)
    print zeichenkette
    # Open a file for writing
    out_file = open("/usr/local/spark/data/test.json","w")
    # Save the dictionary into this file
    # (the 'indent=4' is optional, but makes it more readable)
    json.dump(zeichenkette,out_file) 

################ load documents and  und ################
def Clustering(path, numberClusters, ldaIterations, wordNumbers, name, newPath):
    clusterDestination=newPath
    index = name
    sqlContext = SQLContext(sc)

    ######### load and preprocess data ############
   
    rdd = sc.wholeTextFiles(path).map(lambda (id, text): Row(id= id, text = text.lower().replace(u'ß', 'ss').replace(u'ö', 'oe').replace(u'ü', 'ue').replace(u'ä', 'ae').replace("\n", " ")))
    textDataFrame = sqlContext.createDataFrame(rdd)
    dfsort = textDataFrame.sort('id', ascending=True)
    textDataFrame = dfsort.withColumn("id",  monotonically_increasing_id())   
    textDataFrame.show()

    ################# tokenizer ################

    # pattern: all characters up to '1-9', 'a-z', '-' and '&' are filtered
    regexTokenizer = RegexTokenizer(inputCol="text", outputCol="words", pattern="[^1-9a-z\-\&]")
    regexTokenized = regexTokenizer.transform(textDataFrame)
    
    
    from pyspark.sql.functions import udf
    from pyspark.sql.types import StringType, ArrayType

    # remove integer and strings with length == 1
    def entferneZahlen(liste):
        for i in range(len(liste)):
            if (liste[i].isdigit() == True) or (len(liste[i]) == 1) :
                liste[i] = ''   
            else: liste[i]=liste[i]
        return liste

    dummy_function_udf = udf(entferneZahlen, ArrayType(StringType()))
    df = regexTokenized.withColumn("words", dummy_function_udf(regexTokenized['words']))
    df.show()
    
    regexTokenized = df
    

    ################ stopwordRemover ################

    stopwordNltk = list(stopwords.words('german'))
    
    for i in range(len(stopwordNltk)):
        stopwordNltk[i] = stopwordNltk[i].replace(u'ö', 'oe').replace(u'ü', 'ue').replace(u'ä', 'ae').replace(u'ß', 'ss')
    
    stopwordsSpark = open("/usr/local/spark/data/stopwordsSpark").read().splitlines()
    stopword = open("/usr/local/spark/data/stopwords").read().splitlines()
    stop = stopword + stopwordsSpark + stopwordNltk
    remover = StopWordsRemover(inputCol="words", outputCol="filtered", stopWords = stop)
    clean_text = remover.transform(regexTokenized)
    
   # clean_text.show()
   # clean_text.show(truncate=False)

    ################ vectorizer ################

    #clean = clean_text.select("id" , "filtered")
    #Vector = CountVectorizer(inputCol="filtered", outputCol="vectors")
    #model = Vector.fit(clean)
    #result = model.transform(clean)
    #corpus_size = result.count()  # total number of words
    #corpus = result.select("id", "vectors").map(lambda (x,y): [x,y]).cache()
    
    clean = clean_text.select("id" , "filtered")
    # vectorize
    cv = CountVectorizer(inputCol="filtered", outputCol="vectors")
    model = cv.fit(clean)
    result = model.transform(clean)
    #Now, let's transform the results dataframe back to rdd
    corpus = result.select(F.col('id').cast("long"), 'vectors').rdd.map(lambda x: [x[0], x[1]])
# training data
    lda_model = LDA.train(rdd=corpus, k=numberClusters, seed=12, maxIterations=ldaIterations)
# extracting topics
    topics1 = lda_model.describeTopics(maxTermsPerTopic=wordNumbers)
# extraction vocabulary
    vocabulary = model.vocabulary
    
    scoreli = []
    scorelist = []
    
    for topic in range(len(topics1)):
        print("topic {} : ".format(topic))
        words = topics1[topic][0]
        scores = topics1[topic][1]
        scorelist = []
        for word in range(len(words)):
            print vocabulary[words[word]]
            print scores[word] 
            scorelist.append(scores[word])
        scoreli.append(scorelist)
    print scoreli
    
    
    #corpus = result.select("id", "vectors").map(lambda (x,y): [x,y]).cache()

    ################ LDA Clustering ################
    
    topics = lda_model.topicsMatrix()
    vocabArray = model.vocabulary #2711 zeilen bei allen rezensionen
    topicIndices = sc.parallelize(lda_model.describeTopics(maxTermsPerTopic = wordNumbers))
    topics_final = topicIndices.map(lambda topic: topic_render(topic, vocabArray, wordNumbers)).collect()

    
    for topic in range(len(topics_final)):
        print "Topic" + str(topic)
        for term in topics_final[topic]:
            print term
        print '\n'     
    
    # Save and load model
    lda_model.save(sc, "/usr/local/spark/data/myModelPath")
    ldaModelPath = "/usr/local/spark/data/myPath" 
    sameModel = LDAModel.load(sc, "/usr/local/spark/data/myModelPath")
        

    ################ LDA Clustering ################

    #ldaModel = LDA.train(corpus, k=numberClusters, maxIterations=ldaIterations, optimizer='online')
    #topics = ldaModel.topicsMatrix()
    #vocabArray = model.vocabulary #2711 zeilen bei allen rezensionen
    #topicIndices = sc.parallelize(ldaModel.describeTopics(maxTermsPerTopic = wordNumbers))

    #topics_final = topicIndices.map(lambda topic: topic_render(topic, vocabArray, wordNumbers)).collect()

    #for topic in range(len(topics_final)):
     #   print "Topic" + str(topic)
      #  for term in topics_final[topic]:
       #     print term
       # print '\n' 
    
     # training data
    #lda_model = LDA.train(corpus, k=5, seed=12, maxIterations=50)
    # extracting topics
    #topics = lda_model.describeTopics(maxTermsPerTopic=10)
    # extraction vocabulary
    #vocabulary = model.vocabulary

    assignList = list()
            
    #createDirectories(clusterDestination, numberClusters, index)
    for file in os.listdir(path):
        if file.endswith(""):
            basenameOfFile = os.path.basename(file) 
            pathToSingleDocument = path + "/" + basenameOfFile #kompletterPfadZumDocument

    ######### load and preprocess document ############
            rdd = sc.wholeTextFiles(pathToSingleDocument).map(lambda (id, text): Row(id= id, text = text.lower().replace(u'ß', 'ss').replace(u'ö', 'oe').replace(u'ü', 'ue').replace(u'ä', 'ae').replace("\n", "")))
            textDataFrame = sqlContext.createDataFrame(rdd)
            dfsort = textDataFrame.sort('id', ascending=True)
            textDataFrame = dfsort.withColumn("id",  monotonically_increasing_id())   
 

    ################# tokenizer ################
            
            regexTokenizer = RegexTokenizer(inputCol="text", outputCol="words", pattern="[^1-9a-zA-Z\-\&]")
            regexTokenized = regexTokenizer.transform(textDataFrame)
            
            
            from pyspark.sql.functions import udf
            from pyspark.sql.types import StringType, ArrayType

            #def entferneZahlen(liste):
             #   for i in range(len(liste)):
              #      if (liste[i].isdigit() == True) or (len(liste[i]) == 1) :
               #         liste[i] = ''   
                #    else: liste[i]=liste[i]
             #   return liste

            #dummy_function_udf = udf(entferneZahlen, ArrayType(StringType()))
            #df = regexTokenized.withColumn("words", dummy_function_udf(regexTokenized['words']))
            #df.show()
    
            #regexTokenized = df
            

    ################ stopwordRemover ################

            stopwordNltk = list(stopwords.words('german'))
    
            for i in range(len(stopwordNltk)):
                stopwordNltk[i] = stopwordNltk[i].replace(u'ö', 'oe').replace(u'ü', 'ue').replace(u'ä', 'ae').replace(u'ß', 'ss')
    
            stopwordsSpark = open("/usr/local/spark/data/stopwordsSpark").read().splitlines()
            stopword =  open("/usr/local/spark/data/stopwords").read().splitlines()
            stop = stopword + stopwordsSpark + stopwordNltk
            remover = StopWordsRemover(inputCol="words", outputCol="filtered", stopWords = stop)
            clean_text = remover.transform(regexTokenized)

    ################ Vectorisieren ################

            #clean = clean_text.select("id" , "filtered")
            #Vector = CountVectorizer(inputCol="filtered", outputCol="vectors")
            #model = Vector.fit(clean)
            #result2 = model.transform(clean)
            #corpus_size = result2.count()  # total number of words
            #corpus = result2.select("id", "vectors").map(lambda (x,y): [x,y]).cache()
            
            
            clean = clean_text.select("id" , "filtered")
            Vector = CountVectorizer(inputCol="filtered", outputCol="vectors")
            vectorModel = Vector.fit(clean)
            result2 = vectorModel.transform(clean)
            orpus_size = result2.count()  # total number of words
            corpus2 = result2.select(F.col('id').cast("long"), 'vectors').rdd.map(lambda x: [x[0], x[1]])
                  

    ################ LDA Clustering ################

           # ldaModel = LDA.train(corpus, k=1,maxIterations=10,optimizer='online')
            #topics = ldaModel.topicsMatrix()
           # vocabArray = model.vocabulary 
           # topicIndices = sc.parallelize(ldaModel.describeTopics(maxTermsPerTopic = wordNumbers))
          #  topTopics = topicIndices.map(lambda topic: topic_render(topic, vocabArray, wordNumbers)).collect()
            
            
            # training data
            ldaModel = LDA.train(rdd=corpus2, k=1, seed=12, maxIterations=10)
            topics = ldaModel.topicsMatrix()
            vocabArray = vectorModel.vocabulary #2711 zeilen bei allen rezensionen
            topicIndices = sc.parallelize(ldaModel.describeTopics(maxTermsPerTopic = wordNumbers))
            topTopics = topicIndices.map(lambda topic: topic_render(topic, vocabArray, wordNumbers)).collect()


    ########## select most similar topic #############
    ### bisher bestimmt auf der Algo die Ähnlichkeit nach der maximalen Schnittmenge and topwords, 
    ### toDo: score mit einbeziehen 
            listOfMatchCounts=[None] * numberClusters #list of len(numberClusters)
            if wordNumbers > len(topTopics[0]):
                maxIndex=len(topTopics[0])
            else: maxIndex=wordNumbers
            for u in range (numberClusters):   
                counter=0
                for w in range(maxIndex):
                    for p in range(maxIndex):
                        if topTopics[0][p] == topics_final[u][w]:
                            counter=counter+1                   
                listOfMatchCounts[u]=counter

    ############ save File in Directory ###############      
            maxTopicIndex="/" + str(maxList(listOfMatchCounts))
            print maxTopicIndex
            currentDocument="/" + basenameOfFile  #sollte automatisch gesetzt werden
            newDestination= clusterDestination + "/" + index + "/documents"+ maxTopicIndex + currentDocument
            copyfile(pathToSingleDocument, newDestination)
            print pathToSingleDocument +" saved as " + newDestination
            saveTopTopics(clusterDestination, currentDocument, topTopics, index, maxTopicIndex)
            print "Saved topics from " + pathToSingleDocument + "to" + clusterDestination + "/" + index + "/" + "topics" + maxTopicIndex + currentDocument
            
            # for assignment
            topicDocumentList = []
            topicDocumentList.append(str(maxList(listOfMatchCounts)))
            topicDocumentList.append(basenameOfFile.replace(".txt", ""))
            assignList.append(topicDocumentList)
    
    #Noch eine schönere Endmessage einfügen
    print "Vorgang abgeschlossen"
    print (assignList)
    assignmentTopics = assignment(assignList, numberClusters)
    
    print "topics_final"
    print topics_final
    
    print "scoreli"
    print scoreli
    
    jsonOutput(topics_final, scoreli, ldaModelPath, assignmentTopics, numberClusters, ldaIterations, wordNumbers, path)  
